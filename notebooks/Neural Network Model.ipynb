{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Beer_type Prediction -  Neural Network Model\n",
    "\n",
    "The project is to deploy a Machine Learning model into production. I will train a custom neural networks model that will accurately predict a type of beer based on some rating criterias such as appearance, aroma, palate or taste. I will also build a web app and deploy it online in order to serve my model for real time predictions. This notebook is for designing,training and analysing model.\n",
    "\n",
    "**Student Name:** Wenying Wu \n",
    "\n",
    "**Student Number:** 14007025\n",
    "\n",
    "**Prerequisite:**\n",
    "- Docker\n",
    "- Functions from lab exercise \n",
    "- Datasets saved from Beer_type Prediction - Data Preparation notebook\n",
    "\n",
    "**Sections:**\n",
    "1. Prepare datasets\n",
    "2. Baseline Model\n",
    "3. Define Architecture\n",
    "4. Train & Save Model\n",
    "5. Evaluate model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Prepare Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Load magic command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Import packages and data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data.sets import load_sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_val, y_val, X_test, y_test = load_sets()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Transfer numpy array to tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.pytorch import PytorchDataset\n",
    "\n",
    "train_dataset = PytorchDataset(X=X_train, y=y_train)\n",
    "val_dataset = PytorchDataset(X=X_val, y=y_val)\n",
    "test_dataset = PytorchDataset(X=X_test, y=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([65., 44., 25.,  ..., 13., 12., 29.])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Casual check\n",
    "train_dataset.y_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Baseline Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Load baseline model and get baseline prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.null import NullModel\n",
    "baseline_model = NullModel(target_type='classification')\n",
    "y_base = baseline_model.fit_predict(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Training: 0.07444192974099043\n",
      "F1 Training: 0.010315310209270104\n"
     ]
    }
   ],
   "source": [
    "from src.models.performance import print_class_perf\n",
    "print_class_perf(y_base, y_train, set_name='Training', average='weighted')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** We can see that the accuracy is very low, around 7.4% and F1 score is only 0.01. And the highest beer-type percentage occupied by 'American IPA' is around 7.4% from data preperation notobook command 12."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define Neural Network Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook only shows the final architect used for this project. The trial and error process is not shown here, but is is shown in the drafts folder, you can have a look if you are interested.\n",
    "\n",
    "After trial and error test, the below architecture is chosen as the final model. It is an 8-layer (1 input layer, 1 output layer and 6 hidden layers) feed-forward neural network with dropout and batch-norm. Number of neurons in each layer can be found in below cells."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Import pytorch, define architecture and initialize model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PytorchMultiClass(nn.Module):\n",
    "    def __init__(self, num_features):\n",
    "        super(PytorchMultiClass, self).__init__()\n",
    "        \n",
    "        self.layer_1 = nn.Linear(num_features, 8192) # 2**15\n",
    "        self.layer_2 = nn.Linear(8192, 4096)\n",
    "        self.layer_3 = nn.Linear(4096, 2048)\n",
    "        self.layer_4 = nn.Linear(2048, 1024)\n",
    "        self.layer_5 = nn.Linear(1024, 512)\n",
    "        self.layer_6 = nn.Linear(512, 256)\n",
    "        self.layer_7 = nn.Linear(256, 128) \n",
    "        self.layer_out = nn.Linear(128, 104)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "        self.batchnorm1 = nn.BatchNorm1d(8192)\n",
    "        self.batchnorm2 = nn.BatchNorm1d(4096)\n",
    "        self.batchnorm3 = nn.BatchNorm1d(2048)\n",
    "        self.batchnorm4 = nn.BatchNorm1d(1024)\n",
    "        self.batchnorm5 = nn.BatchNorm1d(512)\n",
    "        self.batchnorm6 = nn.BatchNorm1d(256)\n",
    "        self.batchnorm7 = nn.BatchNorm1d(128)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer_1(x)\n",
    "        x = self.batchnorm1(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        x = self.layer_2(x)\n",
    "        x = self.batchnorm2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.layer_3(x)\n",
    "        x = self.batchnorm3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.layer_4(x)\n",
    "        x = self.batchnorm4(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.layer_5(x)\n",
    "        x = self.batchnorm5(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.layer_6(x)\n",
    "        x = self.batchnorm6(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.layer_7(x)\n",
    "        x = self.batchnorm7(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.layer_out(x)\n",
    "        return x # nn.CrossEntropyLoss does log_softmax() for us so we can simply return x\n",
    "    \n",
    "model = PytorchMultiClass(X_train.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Check if GPU is active"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PytorchMultiClass(\n",
       "  (layer_1): Linear(in_features=6, out_features=8192, bias=True)\n",
       "  (layer_2): Linear(in_features=8192, out_features=4096, bias=True)\n",
       "  (layer_3): Linear(in_features=4096, out_features=2048, bias=True)\n",
       "  (layer_4): Linear(in_features=2048, out_features=1024, bias=True)\n",
       "  (layer_5): Linear(in_features=1024, out_features=512, bias=True)\n",
       "  (layer_6): Linear(in_features=512, out_features=256, bias=True)\n",
       "  (layer_7): Linear(in_features=256, out_features=128, bias=True)\n",
       "  (layer_out): Linear(in_features=128, out_features=104, bias=True)\n",
       "  (softmax): Softmax(dim=1)\n",
       "  (relu): ReLU()\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       "  (batchnorm1): BatchNorm1d(8192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (batchnorm2): BatchNorm1d(4096, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (batchnorm3): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (batchnorm4): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (batchnorm5): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (batchnorm6): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (batchnorm7): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.models.pytorch import get_device\n",
    "device = get_device()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train & Save Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Define 2 dictionaries that will store the accuracy/epoch and loss/epoch for both train and validation sets for later visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_stats = {'train': [], \"val\": []}\n",
    "loss_stats = {'train': [], \"val\": []}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Initialize criterion, optimizer and scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CrossEntropyLoss is used because this is a multiclass classification problem. We don’t have to manually apply a log_softmax layer after our final layer because nn.CrossEntropyLoss does that for us. However, we need to apply log_softmax for validation and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1, gamma=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Set number of EPOCH and batch size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "50 EPOCH is chosen because from the trail and error experience, the model stops to increase it's predicting power after 40~45 epoch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EPOCHS = 50\n",
    "BATCH_SIZE = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "\t(train)\t|\tLoss: 0.0032\t|\tAcc: 19.2%\n",
      "\t(valid)\t|\tLoss: 0.0029\t|\tAcc: 23.6%\n",
      "Epoch: 1\n",
      "\t(train)\t|\tLoss: 0.0030\t|\tAcc: 23.2%\n",
      "\t(valid)\t|\tLoss: 0.0028\t|\tAcc: 27.1%\n",
      "Epoch: 2\n",
      "\t(train)\t|\tLoss: 0.0029\t|\tAcc: 25.4%\n",
      "\t(valid)\t|\tLoss: 0.0026\t|\tAcc: 29.9%\n",
      "Epoch: 3\n",
      "\t(train)\t|\tLoss: 0.0028\t|\tAcc: 27.1%\n",
      "\t(valid)\t|\tLoss: 0.0025\t|\tAcc: 32.5%\n",
      "Epoch: 4\n",
      "\t(train)\t|\tLoss: 0.0027\t|\tAcc: 28.6%\n",
      "\t(valid)\t|\tLoss: 0.0024\t|\tAcc: 34.4%\n",
      "Epoch: 5\n",
      "\t(train)\t|\tLoss: 0.0026\t|\tAcc: 29.7%\n",
      "\t(valid)\t|\tLoss: 0.0024\t|\tAcc: 35.9%\n",
      "Epoch: 6\n",
      "\t(train)\t|\tLoss: 0.0026\t|\tAcc: 30.8%\n",
      "\t(valid)\t|\tLoss: 0.0023\t|\tAcc: 36.7%\n",
      "Epoch: 7\n",
      "\t(train)\t|\tLoss: 0.0026\t|\tAcc: 31.8%\n",
      "\t(valid)\t|\tLoss: 0.0023\t|\tAcc: 37.6%\n",
      "Epoch: 8\n",
      "\t(train)\t|\tLoss: 0.0025\t|\tAcc: 32.7%\n",
      "\t(valid)\t|\tLoss: 0.0022\t|\tAcc: 39.2%\n",
      "Epoch: 9\n",
      "\t(train)\t|\tLoss: 0.0025\t|\tAcc: 33.6%\n",
      "\t(valid)\t|\tLoss: 0.0022\t|\tAcc: 40.4%\n",
      "Epoch: 10\n",
      "\t(train)\t|\tLoss: 0.0025\t|\tAcc: 34.3%\n",
      "\t(valid)\t|\tLoss: 0.0022\t|\tAcc: 41.0%\n",
      "Epoch: 11\n",
      "\t(train)\t|\tLoss: 0.0024\t|\tAcc: 34.9%\n",
      "\t(valid)\t|\tLoss: 0.0021\t|\tAcc: 42.1%\n",
      "Epoch: 12\n",
      "\t(train)\t|\tLoss: 0.0024\t|\tAcc: 35.5%\n",
      "\t(valid)\t|\tLoss: 0.0021\t|\tAcc: 42.5%\n",
      "Epoch: 13\n",
      "\t(train)\t|\tLoss: 0.0024\t|\tAcc: 36.0%\n",
      "\t(valid)\t|\tLoss: 0.0021\t|\tAcc: 43.1%\n",
      "Epoch: 14\n",
      "\t(train)\t|\tLoss: 0.0024\t|\tAcc: 36.4%\n",
      "\t(valid)\t|\tLoss: 0.0021\t|\tAcc: 43.9%\n",
      "Epoch: 15\n",
      "\t(train)\t|\tLoss: 0.0023\t|\tAcc: 36.8%\n",
      "\t(valid)\t|\tLoss: 0.0020\t|\tAcc: 44.7%\n",
      "Epoch: 16\n",
      "\t(train)\t|\tLoss: 0.0023\t|\tAcc: 37.2%\n",
      "\t(valid)\t|\tLoss: 0.0020\t|\tAcc: 44.8%\n",
      "Epoch: 17\n",
      "\t(train)\t|\tLoss: 0.0023\t|\tAcc: 37.6%\n",
      "\t(valid)\t|\tLoss: 0.0020\t|\tAcc: 45.6%\n",
      "Epoch: 18\n",
      "\t(train)\t|\tLoss: 0.0023\t|\tAcc: 37.8%\n",
      "\t(valid)\t|\tLoss: 0.0020\t|\tAcc: 45.5%\n",
      "Epoch: 19\n",
      "\t(train)\t|\tLoss: 0.0023\t|\tAcc: 38.2%\n",
      "\t(valid)\t|\tLoss: 0.0020\t|\tAcc: 46.2%\n",
      "Epoch: 20\n",
      "\t(train)\t|\tLoss: 0.0023\t|\tAcc: 38.4%\n",
      "\t(valid)\t|\tLoss: 0.0020\t|\tAcc: 46.5%\n",
      "Epoch: 21\n",
      "\t(train)\t|\tLoss: 0.0023\t|\tAcc: 38.7%\n",
      "\t(valid)\t|\tLoss: 0.0019\t|\tAcc: 46.9%\n",
      "Epoch: 22\n",
      "\t(train)\t|\tLoss: 0.0023\t|\tAcc: 38.9%\n",
      "\t(valid)\t|\tLoss: 0.0019\t|\tAcc: 47.3%\n",
      "Epoch: 23\n",
      "\t(train)\t|\tLoss: 0.0023\t|\tAcc: 39.1%\n",
      "\t(valid)\t|\tLoss: 0.0019\t|\tAcc: 47.7%\n",
      "Epoch: 24\n",
      "\t(train)\t|\tLoss: 0.0022\t|\tAcc: 39.2%\n",
      "\t(valid)\t|\tLoss: 0.0019\t|\tAcc: 48.0%\n",
      "Epoch: 25\n",
      "\t(train)\t|\tLoss: 0.0022\t|\tAcc: 39.4%\n",
      "\t(valid)\t|\tLoss: 0.0019\t|\tAcc: 47.8%\n",
      "Epoch: 26\n",
      "\t(train)\t|\tLoss: 0.0022\t|\tAcc: 39.6%\n",
      "\t(valid)\t|\tLoss: 0.0019\t|\tAcc: 48.3%\n",
      "Epoch: 27\n",
      "\t(train)\t|\tLoss: 0.0022\t|\tAcc: 39.7%\n",
      "\t(valid)\t|\tLoss: 0.0019\t|\tAcc: 48.5%\n",
      "Epoch: 28\n",
      "\t(train)\t|\tLoss: 0.0022\t|\tAcc: 40.0%\n",
      "\t(valid)\t|\tLoss: 0.0019\t|\tAcc: 48.5%\n",
      "Epoch: 29\n",
      "\t(train)\t|\tLoss: 0.0022\t|\tAcc: 40.0%\n",
      "\t(valid)\t|\tLoss: 0.0019\t|\tAcc: 48.9%\n",
      "Epoch: 30\n",
      "\t(train)\t|\tLoss: 0.0022\t|\tAcc: 40.2%\n",
      "\t(valid)\t|\tLoss: 0.0019\t|\tAcc: 48.8%\n",
      "Epoch: 31\n",
      "\t(train)\t|\tLoss: 0.0022\t|\tAcc: 40.2%\n",
      "\t(valid)\t|\tLoss: 0.0019\t|\tAcc: 48.9%\n",
      "Epoch: 32\n",
      "\t(train)\t|\tLoss: 0.0022\t|\tAcc: 40.3%\n",
      "\t(valid)\t|\tLoss: 0.0019\t|\tAcc: 49.1%\n",
      "Epoch: 33\n",
      "\t(train)\t|\tLoss: 0.0022\t|\tAcc: 40.4%\n",
      "\t(valid)\t|\tLoss: 0.0019\t|\tAcc: 49.1%\n",
      "Epoch: 34\n",
      "\t(train)\t|\tLoss: 0.0022\t|\tAcc: 40.5%\n",
      "\t(valid)\t|\tLoss: 0.0019\t|\tAcc: 49.3%\n",
      "Epoch: 35\n",
      "\t(train)\t|\tLoss: 0.0022\t|\tAcc: 40.6%\n",
      "\t(valid)\t|\tLoss: 0.0019\t|\tAcc: 49.3%\n",
      "Epoch: 36\n",
      "\t(train)\t|\tLoss: 0.0022\t|\tAcc: 40.6%\n",
      "\t(valid)\t|\tLoss: 0.0019\t|\tAcc: 49.4%\n",
      "Epoch: 37\n",
      "\t(train)\t|\tLoss: 0.0022\t|\tAcc: 40.6%\n",
      "\t(valid)\t|\tLoss: 0.0019\t|\tAcc: 49.4%\n",
      "Epoch: 38\n",
      "\t(train)\t|\tLoss: 0.0022\t|\tAcc: 40.7%\n",
      "\t(valid)\t|\tLoss: 0.0019\t|\tAcc: 49.4%\n",
      "Epoch: 39\n",
      "\t(train)\t|\tLoss: 0.0022\t|\tAcc: 40.8%\n",
      "\t(valid)\t|\tLoss: 0.0019\t|\tAcc: 49.6%\n",
      "Epoch: 40\n",
      "\t(train)\t|\tLoss: 0.0022\t|\tAcc: 40.8%\n",
      "\t(valid)\t|\tLoss: 0.0019\t|\tAcc: 49.5%\n",
      "Epoch: 41\n",
      "\t(train)\t|\tLoss: 0.0022\t|\tAcc: 40.8%\n",
      "\t(valid)\t|\tLoss: 0.0018\t|\tAcc: 49.7%\n",
      "Epoch: 42\n",
      "\t(train)\t|\tLoss: 0.0022\t|\tAcc: 40.9%\n",
      "\t(valid)\t|\tLoss: 0.0018\t|\tAcc: 49.7%\n",
      "Epoch: 43\n",
      "\t(train)\t|\tLoss: 0.0022\t|\tAcc: 40.9%\n",
      "\t(valid)\t|\tLoss: 0.0018\t|\tAcc: 49.7%\n",
      "Epoch: 44\n",
      "\t(train)\t|\tLoss: 0.0022\t|\tAcc: 40.9%\n",
      "\t(valid)\t|\tLoss: 0.0018\t|\tAcc: 49.8%\n",
      "Epoch: 45\n",
      "\t(train)\t|\tLoss: 0.0022\t|\tAcc: 41.0%\n",
      "\t(valid)\t|\tLoss: 0.0018\t|\tAcc: 49.8%\n",
      "Epoch: 46\n",
      "\t(train)\t|\tLoss: 0.0022\t|\tAcc: 41.0%\n",
      "\t(valid)\t|\tLoss: 0.0018\t|\tAcc: 49.8%\n",
      "Epoch: 47\n",
      "\t(train)\t|\tLoss: 0.0022\t|\tAcc: 41.0%\n",
      "\t(valid)\t|\tLoss: 0.0018\t|\tAcc: 49.8%\n",
      "Epoch: 48\n",
      "\t(train)\t|\tLoss: 0.0022\t|\tAcc: 41.1%\n",
      "\t(valid)\t|\tLoss: 0.0018\t|\tAcc: 49.8%\n",
      "Epoch: 49\n",
      "\t(train)\t|\tLoss: 0.0022\t|\tAcc: 41.1%\n",
      "\t(valid)\t|\tLoss: 0.0018\t|\tAcc: 49.8%\n"
     ]
    }
   ],
   "source": [
    "#use train_classification, test_classification defined in lab5\n",
    "from src.models.pytorch import train_classification, test_classification\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    train_loss, train_acc = train_classification(train_dataset, model=model, criterion=criterion, optimizer=optimizer, batch_size=BATCH_SIZE, device=device, scheduler=scheduler, accuracy_stats=accuracy_stats, loss_stats=loss_stats, shuffle=True)\n",
    "    valid_loss, valid_acc = test_classification(val_dataset, model=model, criterion=criterion, batch_size=BATCH_SIZE, device=device, accuracy_stats=accuracy_stats, loss_stats=loss_stats)\n",
    "\n",
    "    print(f'Epoch: {epoch}')\n",
    "    print(f'\\t(train)\\t|\\tLoss: {train_loss:.4f}\\t|\\tAcc: {train_acc * 100:.1f}%')\n",
    "    print(f'\\t(valid)\\t|\\tLoss: {valid_loss:.4f}\\t|\\tAcc: {valid_acc * 100:.1f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Experiment log:**\n",
    "- lab 5 architecture: train 16.5 | val 16.7 | test ~20 (30 EPOCH, no batchnorm)\n",
    "- lab 5 architecture + SMOTE Dataset: train 15.6 | val 16.2 | test ~20 (30 EPOCH, no batchnorm)\n",
    "- lab 5 architecture + Oversampling Dataset: train 30.1 | val 21.6 | test ~30 (30 EPOCH, no batchnorm) - oversampling tends to overfit the model\n",
    "- 2 layers(6-108-104): train 20.2 | val 21.1 | test ~30 (30 EPOCH)\n",
    "- 2 layers(6-55-104): train 19.3 | val 20.3 | test ~30 (30 EPOCH)\n",
    "- 4 layers(6-512-256-128-104): train 30.9 | val 37.4 | test ~37 (30 EPOCH)\n",
    "- 6 layers(6-1024-512-256-128-64-104): train 33.1 | val 40.7 | test 40.5 (100 EPOCH)\n",
    "- 7 layers(6-2048-1024-512-256-128-64-104): train 33.8 | val 41.3 | test 41.0 (100 EPOCH)\n",
    "- 7 layers(6-2048-1024-512-256-128-108-104): train 35.1 | val 43.0 | test 42.7 (seems like more neurons on the last layer improves) (100 EPOCH)\n",
    "- 7 layers(6-2048-1512-1024-512-256-128-104): train 39.8 | val 47.7 | test 47.5 (seems like more neurons on the all layers improves) (100 EPOCH)\n",
    "- 11 layers(6-4096-3584-3072-2560-2048-1536-1024-512-256-128-104): train 40.7 | val 48.4 | test 48.7 (loss:0.0022, 0.0019,0.0019, higher than previous 0.0003, more reasonable) (28 epoch, accidently stopped)\n",
    "- 8 layers(6-8192-4096-2048-1024-512-256-128-104): train 41.1 | val 49.8 | test 49.5 (50 EPOCH)  \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Save model state_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the state_dict instead of whole model. According to https://pytorch.org/tutorials/beginner/saving_loading_models.html :\n",
    "Saving a whole model will save the entire module using Python’s pickle module. The disadvantage of this approach is that the serialized data is bound to the specific classes and the exact directory structure used when the model is saved. The reason for this is because pickle does not save the model class itself. Rather, it saves a path to the file containing the class, which is used during load time. Because of this, your code can break in various ways when used in other projects or after refactors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save statedict instead of whole model (pytorch official recommended way)\n",
    "torch.save(model.state_dict(), \"../models/nn_final_dict.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluate Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Predict on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 0.0018\t|\tAccuracy: 0.495\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc, y_pred_list = test_classification(test_dataset, model=model, criterion=criterion, batch_size=BATCH_SIZE, device=device, accuracy_stats=accuracy_stats, loss_stats=loss_stats)\n",
    "print(f'\\tLoss: {test_loss:.4f}\\t|\\tAccuracy: {test_acc:.3f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
